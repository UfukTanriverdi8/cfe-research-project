{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ed0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os, json, random, re\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key\n",
    "OpenAI_Client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "EVAL_MODEL = \"gpt-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a581d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_candidate_scores(cands_str: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Parse 'cand_1 (score)|cand_2 (score)|...' -> [score1, score2, ...] as floats.\n",
    "    Ignores segments that don't have a '(number)'.\n",
    "    \"\"\"\n",
    "    if pd.isna(cands_str):\n",
    "        return []\n",
    "    parts = [p.strip() for p in str(cands_str).split('|') if p.strip()]\n",
    "    scores: List[float] = []\n",
    "    pat = re.compile(r'\\(([-+]?\\d*\\.?\\d+)\\)')  # capture the number inside the last parentheses\n",
    "    for part in parts:\n",
    "        m = pat.findall(part)\n",
    "        if m:\n",
    "            scores.append(float(m[-1]))\n",
    "    return scores\n",
    "\n",
    "# ---------- knee / drop utilities ----------\n",
    "\n",
    "def knee_index_point_before_largest_drop(raw_scores: List[float]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Given raw candidate scores, sort them DESC and return the index i such that\n",
    "    the drop between sorted[i] and sorted[i+1] is maximal. If ties, return the first.\n",
    "    Returns 0 for single-element lists. Returns None for empty lists.\n",
    "    \"\"\"\n",
    "    if not raw_scores:\n",
    "        return None\n",
    "    s = sorted(raw_scores, reverse=True)\n",
    "    if len(s) == 1:\n",
    "        return 0\n",
    "    drops = [s[i] - s[i+1] for i in range(len(s)-1)]\n",
    "    # tie-breaker: first largest drop (lowest i)\n",
    "    i_max = max(range(len(drops)), key=lambda i: drops[i])\n",
    "    return i_max  # \"point before\" means we include index i in the mass\n",
    "\n",
    "# ---------- CI computation (per-row) ----------\n",
    "\n",
    "def compute_ci_from_scores(raw_scores: List[float], scale: float = 1000.0) -> float:\n",
    "    \"\"\"\n",
    "    CI = (M * S) / scale\n",
    "      - S: RAW sum of all candidate scores (0..100 each; with 10 cands max S is 1000)\n",
    "      - M: sum of NORMALIZED scores (score/S) up to and including the knee index\n",
    "    Returns np.nan if scores are empty or sum is zero.\n",
    "    \"\"\"\n",
    "    if not raw_scores:\n",
    "        return np.nan\n",
    "    S = float(sum(raw_scores))\n",
    "    if S <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    k = knee_index_point_before_largest_drop(raw_scores)\n",
    "    if k is None:\n",
    "        return np.nan\n",
    "\n",
    "    # normalize by total RAW sum S (i.e., turn into a probability mass over candidates)\n",
    "    s_desc = sorted(raw_scores, reverse=True)\n",
    "    norm_desc = [x / S for x in s_desc]\n",
    "    M = float(sum(norm_desc[:k+1]))  # include the knee index itself\n",
    "\n",
    "    CI = (M * S) / float(scale)\n",
    "    return CI\n",
    "\n",
    "def compute_ci_from_str(cands_str: str, scale: float = 1000.0) -> float:\n",
    "    return compute_ci_from_scores(parse_candidate_scores(cands_str), scale=scale)\n",
    "\n",
    "# ---------- simple filtering ----------\n",
    "\n",
    "def simple_filtering(df: pd.DataFrame,\n",
    "                     cand_col: str = \"candidate_answers\",\n",
    "                     threshold: float = 50.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds boolean column 'simple_conf' and returns a *copy* of df.\n",
    "    - If ge50_means_nonconf=True (your latest instruction): simple_conf = False when any score >= 50.\n",
    "      i.e., >=50 => NON-confusing\n",
    "    - If ge50_means_nonconf=False (earlier slide): simple_conf = True when any score >= 50.\n",
    "      i.e., >=50 => confusing\n",
    "    \"\"\"\n",
    "    def flag(cands_str: str) -> bool:\n",
    "        scores = parse_candidate_scores(cands_str)\n",
    "        print(f\"{scores}\")\n",
    "        return scores[0] > threshold\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"simple_conf\"] = out[cand_col].apply(flag)\n",
    "    return out\n",
    "\n",
    "# ---------- global CI threshold (precompute once) ----------\n",
    "\n",
    "def compute_ci_series(df: pd.DataFrame,\n",
    "                      cand_col: str = \"candidate_answers\",\n",
    "                      scale: float = 1000.0) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute CI per row; returns a float Series (index aligned with df).\n",
    "    \"\"\"\n",
    "    return df[cand_col].apply(lambda s: compute_ci_from_str(s, scale=scale))\n",
    "\n",
    "def knee_threshold_over_ci(ci_series: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Sort CI values DESC, find the point before the largest drop, and return that CI value as threshold.\n",
    "    Ties -> pick the first (earliest) in the DESC list.\n",
    "    Returns np.nan if not enough values.\n",
    "    \"\"\"\n",
    "    vals = ci_series.dropna().sort_values(ascending=False).tolist()\n",
    "    if not vals:\n",
    "        return np.nan\n",
    "    if len(vals) == 1:\n",
    "        return vals[0]\n",
    "    drops = [vals[i] - vals[i+1] for i in range(len(vals)-1)]\n",
    "    i_max = max(range(len(drops)), key=lambda i: drops[i])\n",
    "    return vals[i_max]  # the value *before* the largest drop\n",
    "\n",
    "# ---------- advanced filtering ----------\n",
    "\n",
    "def advanced_filtering(df: pd.DataFrame,\n",
    "                       ci_threshold: float,\n",
    "                       cand_col: str = \"candidate_answers\",\n",
    "                       scale: float = 1000.0,\n",
    "                       add_ci_column: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds boolean column 'adv_conf' using global ci_threshold and returns a *copy* of df.\n",
    "    Also adds a 'CI' column if add_ci_column=True.\n",
    "    Rule: adv_conf = (CI >= ci_threshold)\n",
    "    \"\"\"\n",
    "    CIs = compute_ci_series(df, cand_col=cand_col, scale=scale)\n",
    "    out = df.copy()\n",
    "    if add_ci_column:\n",
    "        out[\"CI\"] = CIs\n",
    "    out[\"adv_conf\"] = CIs >= ci_threshold\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4972360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data with 45 rows.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/results/test_100q_results.csv\")\n",
    "print(\"Loaded test data with\", len(df), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f9d3e",
   "metadata": {},
   "source": [
    "### Accuracy Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee978bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last time the Philadelphia Eagles went to the Super Bowl was in 2018. They played in Super Bowl LII (52) against the New England Patriots at U.S. Bank Stadium in Minneapolis, Minnesota. The Eagles won that game with a score of 41-33.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: A great question for all the Eagles fans out there!\n",
      "\n",
      "The Philadelphia Eagles last appeared in the Super Bowl in 2023, when they played in Super Bowl LVII (57) against the Kansas City Chiefs on February 12, 2023. Unfortunately, they lost the game 38-35.\n",
      "\n",
      "However, if you're thinking of a more recent or specific year, I'd be happy to try and help you with that!\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in the 2022 season, which was Super Bowl LVII (57) held on February 12, 2023. They played against the Kansas City Chiefs and lost the game. Prior to that, their most recent Super Bowl appearance was Super Bowl LII (52) on February 4, 2018, where they defeated the New England Patriots.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in 2023, when they played in Super Bowl LVII (57) against the Kansas City Chiefs on February 12, 2023.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last time the Philadelphia Eagles went to the Super Bowl was in the 2022 season, which culminated in Super Bowl LVII on February 12, 2023. They played against the Kansas City Chiefs.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last year the Philadelphia Eagles went to the Super Bowl was 2018, where they defeated the New England Patriots in Super Bowl LII.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in 2023, where they played against the Kansas City Chiefs in Super Bowl LVII (57). They lost the game 38-35.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in the 2022 season, which was Super Bowl LVII (57) held on February 12, 2023. They played against the Kansas City Chiefs and lost the game.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in 2023, when they played in Super Bowl LVII (57) against the Kansas City Chiefs on February 12, 2023.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in the 2022 season, which culminated in Super Bowl LVII on February 12, 2023.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last year the Philadelphia Eagles went to the Super Bowl was 2018, where they lost to the Atlanta Falcons in Super Bowl LIII.\n",
      "\n",
      "This clarifies the misconception that they might have gone to the Super Bowl more recently, as 2018 is the most recent Super Bowl appearance for the Eagles.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last year the Philadelphia Eagles went to the Super Bowl was 2023.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last year the Philadelphia Eagles went to the Super Bowl was 2017, when they won Super Bowl LII against the New England Patriots. Some people might think it was more recent, but that is the most recent Super Bowl appearance for the Eagles.\n",
      "Verdict: YES - True\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The Philadelphia Eagles last went to the Super Bowl in 2023, when they played in Super Bowl LVII (57) against the Kansas City Chiefs. Some people might think it was 2018, which is also correct, as the Eagles did win Super Bowl LII (52) that year by defeating the New England Patriots. However, the question asks for the last year they went to the Super Bowl, which would be 2023.\n",
      "Verdict: NO - False\n",
      "Question: when was the last year the eagles went to the superbowl?\n",
      "Gold: 2017\n",
      "Model Response: The last year the Philadelphia Eagles went to the Super Bowl was in 2023. They played in Super Bowl LVII. \n",
      "\n",
      "Some might mistakenly think of their previous appearance in 2018, when they won Super Bowl LII, but their most recent appearance was indeed in 2023.\n",
      "Verdict: NO - False\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in Absolutely Fabulous is played by Renée Elise Goldsbery. However, it's worth noting that the character of Bubble was originally portrayed by Jane Horrocks in the first series of the show. Renée Elise Goldsbery took over the role for the second series and beyond.\n",
      "Verdict: NO - False\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in the British sitcom \"Absolutely Fabulous\" is played by actress Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: In the television series \"Absolutely Fabulous\" (often abbreviated as \"Ab Fab\"), the character of Bubble is played by actress Jane Horrocks. Bubble is known for her eccentric and flamboyant personality, often providing comic relief with her outrageous outfits and naive yet endearing demeanor. Jane Horrocks has been praised for her portrayal of this unique and memorable character.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in the British sitcom \"Absolutely Fabulous\" is played by actress Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in \"Absolutely Fabulous\" is played by actress Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in \"Absolutely Fabulous\" is played by Jennifer Saunders. She also co-created the show and is known for her comedic performances in the series.\n",
      "Verdict: NO - False\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The character Bubble is played by actress Jane Horrocks in the British sitcom Absolutely Fabulous.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in the television series \"Absolutely Fabulous\" is played by Jane Horrocks. Bubble is known for her eccentric and quirky personality, often providing comic relief with her unusual fashion sense and naive outlook.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in Absolutely Fabulous is played by Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in \"Absolutely Fabulous\" is played by Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in Absolutely Fabulous is played by Jennifer Saunders. Jennifer Saunders created the character and portrayed Bubble throughout the series.\n",
      "Verdict: NO - False\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in Absolutely Fabulous is played by Jane Horrocks.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in Absolutely Fabulous is played by Jane Horrocks. She portrays Bubble as a ditzy and eccentric personal assistant to Edina Monsoon, known for her quirky fashion sense and often nonsensical dialogue.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble is played by Jane Horrocks. Some people might confuse Bubble with other characters, such as Saffron, who is also a part of Edina's life, but Bubble is specifically Edina's eccentric and somewhat dim-witted personal assistant.\n",
      "Verdict: YES - True\n",
      "Question: Who plays the role of Bubble in Absolutely Fabulous?\n",
      "Gold: Jane Horrocks\n",
      "Model Response: The role of Bubble in \"Absolutely Fabulous\" is played by Jane Horrocks. \n",
      "\n",
      "Some might mistakenly think that Jennifer Saunders, who created the show and plays the main character Edina Monsoon, also plays Bubble. However, Jane Horrocks is the actress who brings the quirky and eccentric character of Bubble to life.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is a deep, bright red. This signature red sole is one of the brand's most recognizable features and is often associated with luxury and elegance in the fashion world.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red. This signature red sole has become a hallmark of the brand and is often referred to as \"Louboutin red.\" It's a bright, fire engine red that is instantly recognizable and has become synonymous with high-end fashion.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is a vibrant red. This signature red sole has become one of the most recognizable features of Louboutin footwear, symbolizing luxury and high fashion. The specific shade of red used is often referred to as \"Louboutin red\" and is protected by trademark in several countries.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive colour of the soles of shoes designed by Christian Louboutin is red.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is a bright red. This signature red sole is one of the brand's most recognizable features.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red. This signature red sole has become one of the most recognizable features of Louboutin footwear.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is a deep red. This signature red sole is one of the brand's most recognizable features.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: Red.\n",
      "\n",
      "Clarification: The distinctive red sole is a signature feature of Christian Louboutin's shoe designs, and is a key part of the brand's visual identity. The red sole is a deliberate design choice, intended to add a touch of luxury and glamour to the shoes. It's worth noting that not all of Louboutin's shoes have red soles - some designs may feature different colors or no sole color at all. However, the red sole is a hallmark of the brand's high-end aesthetic.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red. This vibrant red sole has become a signature feature of Louboutin's footwear, making them easily recognizable and highly sought after. Some people might think that the soles could be black or another color, but the iconic red is the correct and distinctive color associated with Louboutin shoes.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive colour of the soles of shoes designed by Christian Louboutin is red. Some people might think it's just a common design element, but the red sole has actually become a trademark feature of Louboutin's designs, symbolizing luxury and high-end fashion. The red sole is often seen as a status symbol and is instantly recognizable as a signature element of Louboutin's brand.\n",
      "Verdict: YES - True\n",
      "Question: What is the distinctive colour of the soles of shoes designed by Christian Louboutin?\n",
      "Gold: Red\n",
      "Model Response: The distinctive color of the soles of shoes designed by Christian Louboutin is red. Some might mistakenly think of other luxury brands that use different signature colors or designs, but the red sole is a trademark of Christian Louboutin shoes, making them easily recognizable.\n",
      "Verdict: YES - True\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(question: str, gold: str, response: str):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant that evaluates the correctness of model answers that are given to different questions.\n",
    "    Your task is to determine if the model's answer correctly contains the gold answer.\n",
    "    You will be given a question, a gold answer(actually correct answer), and a model answer.\n",
    "    Your response should be in the following format:\n",
    "    In one sentence, explain how the model answer compares to the gold answer.\n",
    "    On a new line, output exactly YES if the model answer correctly contains the gold answer, otherwise NO.\n",
    "    IF YOU THINK THAT GIVEN GOLD ANSWER IS NOT CORRECT, YOU SHOULD FIRST PROVIDE AN EXPLANATION WHY IT IS NOT CORRECT AND THEN OUTPUT N/A.\n",
    "    \"\"\" \n",
    "    user_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Gold answer: \"{gold}\"\n",
    "    Model answer: \"{response}\"\n",
    "\n",
    "    Is the model answer correct based on the gold answer? Only output YES or NO.\n",
    "    \"\"\"\n",
    "    res = OpenAI_Client.chat.completions.create(\n",
    "        model=EVAL_MODEL,\n",
    "        messages=[\n",
    "            # we will not use the system prompt for this basic task\n",
    "            {\"role\": \"user\", \"content\": user_prompt}]\n",
    "                    )\n",
    "    verdict = res.choices[0].message.content.strip()\n",
    "    print(f\"Question: {question}\\nGold: {gold}\\nModel Response: {response}\\nVerdict: {verdict} - {verdict == 'YES'}\")\n",
    "    return verdict == \"YES\"\n",
    "\n",
    "df[['accuracy']] = df.apply(\n",
    "    lambda r: pd.Series(\n",
    "        evaluate_accuracy(r['question'], r['gold_answer'], r['llm_response'])\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e11d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/results/test_100q_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3139e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 28 rows with accuracy == True.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_acc = df[df[\"accuracy\"] == True].copy()\n",
    "print(\"Filtered to\", len(df_acc), \"rows with accuracy == True.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f0051",
   "metadata": {},
   "source": [
    "### Confusability Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d0333c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80.0, 60.0, 55.0, 50.0, 45.0, 40.0, 30.0, 25.0, 20.0, 10.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[20.0, 18.0, 15.0, 12.0, 10.0, 9.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "[22.0, 20.0, 18.0, 15.0, 12.0, 10.0, 8.0, 6.0, 5.0, 4.0]\n",
      "After simple filtering, confusing rows: 1, non-confusing rows: 27\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # simple filtering (latest spec: >=50 => NON-confusing, so confusing=True when NOT any >=50)\n",
    "df_acc = simple_filtering(df_acc, cand_col=\"candidate_answers\", threshold=50.0)\n",
    "print(f\"After simple filtering, confusing rows: {len(df_acc[df_acc['simple_conf'] == True])}, non-confusing rows: {len(df_acc[df_acc['simple_conf'] == False])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cb5f6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "questions: 100%|██████████| 1266/1266 [00:00<00:00, 90364.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded global questions with 1266 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the whole json file\n",
    "with open(\"../data/questions/1200_verified_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    global_questions = json.load(f)\n",
    "from tqdm import tqdm\n",
    "rows = []\n",
    "for q in tqdm(global_questions, desc=\"questions\"):\n",
    "        question_text   = q[\"question\"]\n",
    "        correct_answer  = q[\"answer\"]\n",
    "        cand_dict = q.get(\"candidate_answers\", {})\n",
    "        # sort answer texts by their listwise score descending\n",
    "        candidate_ans = sorted(\n",
    "            cand_dict.keys(),\n",
    "            key=lambda ans: cand_dict[ans].get(\"listwise\", 0),\n",
    "            reverse=True\n",
    "        )\n",
    "        # add listwise scores to the candidates\n",
    "        candidate_ans_scores = [\n",
    "            f\"{ans} ({cand_dict[ans].get('listwise', 0)})\"\n",
    "            for ans in candidate_ans\n",
    "        ]\n",
    "\n",
    "        user_prompt = (\n",
    "            f\"{question_text}\"\n",
    "        )\n",
    "                \n",
    "        rows.append(\n",
    "            {\n",
    "                \"id\"               : q[\"id\"],\n",
    "                \"question\"         : question_text,\n",
    "                \"gold_answer\"      : correct_answer,\n",
    "                \"candidate_answers\": \"|\".join(candidate_ans),\n",
    "            }\n",
    "        )\n",
    "df_global = pd.DataFrame(rows)\n",
    "print(\"Loaded global questions with\", len(df_global), \"rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3b24e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed CI threshold: 1.2\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4      NaN\n",
      "        ..\n",
      "1261   NaN\n",
      "1262   NaN\n",
      "1263   NaN\n",
      "1264   NaN\n",
      "1265   NaN\n",
      "Name: candidate_answers, Length: 1266, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ci_series = compute_ci_series(df_global, cand_col=\"candidate_answers\", scale=1000.0)\n",
    "ci_threshold = knee_threshold_over_ci(ci_series)\n",
    "print(f\"Computed CI threshold: {ci_threshold}\")\n",
    "print(ci_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc = advanced_filtering(df_acc, ci_threshold=ci_threshold, cand_col=\"candidate_answers\", scale=1000.0, add_ci_column=True)\n",
    "print(f\"After advanced filtering, confusing rows: {len(df_acc[df_acc['adv_conf'] == True])}, non-confusing rows: {len(df_acc[df_acc['adv_conf'] == False])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e76e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8217f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"../data/eval_results/100q_results_ambigious.csv\"\n",
    "df = pd.read_csv(path)\n",
    "# change the name of the column 1st_layer_score and 1st_layer_explanation to gpt_conf_score and gpt_conf_explanation\n",
    "df.rename(columns={\"1st_layer_score\": \"gpt_conf_score\", \"1st_layer_explanation\": \"gpt_conf_explanation\"}, inplace=True)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24fab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions in the first GPT evaluation results: 88\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1861 entries, 0 to 1860\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     1861 non-null   object \n",
      " 1   prompt_variant         1861 non-null   object \n",
      " 2   model                  1861 non-null   object \n",
      " 3   question               1861 non-null   object \n",
      " 4   gold_answer            1861 non-null   object \n",
      " 5   candidate_answers      1861 non-null   object \n",
      " 6   llm_response           1861 non-null   object \n",
      " 7   response_length        1861 non-null   int64  \n",
      " 8   human_score            0 non-null      float64\n",
      " 9   accuracy               1861 non-null   bool   \n",
      " 10  simple_conf            1861 non-null   bool   \n",
      " 11  CI                     1861 non-null   float64\n",
      " 12  adv_conf               1861 non-null   bool   \n",
      " 13  mentioned_cands        1861 non-null   object \n",
      " 14  simple_metric          1861 non-null   float64\n",
      " 15  adv_metric             1861 non-null   float64\n",
      " 16  just_reward            1861 non-null   float64\n",
      " 17  num_mentions           1861 non-null   int64  \n",
      " 18  1st_layer_score        1861 non-null   int64  \n",
      " 19  1st_layer_explanation  1861 non-null   object \n",
      " 20  gpt_candidate_answers  88 non-null     object \n",
      "dtypes: bool(3), float64(5), int64(3), object(10)\n",
      "memory usage: 267.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_2st_gpt = pd.read_csv(\"../data/results/100q_with_gpt_candidates.csv\")\n",
    "unique_questions = df_2st_gpt['question'].nunique()\n",
    "print(f\"Number of unique questions in the first GPT evaluation results: {unique_questions}\")\n",
    "df_2st_gpt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d30c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os, json, random, re\n",
    "import ast\n",
    "import numpy as np\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key\n",
    "OpenAI_Client = openai.OpenAI(\n",
    "    default_headers={\n",
    "        \"HTTP-Referer\": \"cfe-paper\",\n",
    "    },\n",
    "    #base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),   \n",
    ")\n",
    "\n",
    "OpenRouter_Client = openai.OpenAI(\n",
    "    default_headers={\n",
    "        \"HTTP-Referer\": \"cfe-paper\",\n",
    "    },\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),   \n",
    ")\n",
    "EVAL_MODEL = \"openai/gpt-5-2025-08-07\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a934f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_response(system_prompt, user_prompt, model=EVAL_MODEL, reasoning_effort=\"medium\"):\n",
    "    res = OpenRouter_Client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format={\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        reasoning_effort=reasoning_effort)\n",
    "    \n",
    "    response_text = res.choices[0].message.content.strip()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedd1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the intersection of mentioned_cands and gpt_candidate_answers \n",
    "# by based on the phrasing of the options of gpt_candidate_answers\n",
    "INTERSECTION_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert semantic evaluator.\n",
    "Your task is to find the semantic intersection between two lists of entity names. \n",
    "You will be provided with List A and List B.\n",
    "### Instructions:\n",
    "- Identify entities that are semantically equivalent between the two lists.\n",
    "- We consider entities to be semantically equivalent if they are the abbreviated, synonymous, or slight variations of each other that represent the same thing.\n",
    "- Return a JSON array containing the names of the entities that are present in both lists based on semantic similarity.\n",
    "- Ensure that the output is a valid JSON array.\n",
    "### Output format:\n",
    "[\n",
    "  \"<entity name 1>\",\n",
    "  \"<entity name 2>\",\n",
    "  ...\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faf099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
