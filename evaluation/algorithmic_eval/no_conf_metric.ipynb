{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b2f24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os, json, random, re\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10923f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/results/100q_results_all.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a9441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "S_MAX = 825.0  # given by you; do not recompute\n",
    "\n",
    "# ----------------------------\n",
    "# Parsing helpers (from your old metric code)\n",
    "# ----------------------------\n",
    "_cand_re = re.compile(r\"\\s*(?P<label>.+?)\\s*\\(\\s*(?P<score>-?\\d+(?:\\.\\d+)?)\\s*\\)\\s*$\")\n",
    "\n",
    "def parse_candidate_scores(cand_str: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Parse 'candidate_answers' like: \"Paris (87)|Lyon (42)|Marseille (15)\" -> { \"Paris\":87.0, ... }\n",
    "    Returns empty dict on bad/missing strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(cand_str, str) or not cand_str.strip():\n",
    "        return {}\n",
    "    out = {}\n",
    "    for chunk in cand_str.split(\"|\"):\n",
    "        m = _cand_re.match(chunk)\n",
    "        if m:\n",
    "            lab = m.group(\"label\").strip()\n",
    "            try:\n",
    "                out[lab] = float(m.group(\"score\"))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return out\n",
    "\n",
    "def _canon(s: str) -> str:\n",
    "    \"\"\"Lowercase + collapse whitespace for robust matching.\"\"\"\n",
    "    return \" \".join(str(s).lower().split())\n",
    "\n",
    "def parse_mentioned(val) -> List[str]:\n",
    "    \"\"\"\n",
    "    'mentioned_cands' may be a real list or a stringified list.\n",
    "    Returns [] if missing.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return [str(x) for x in val]\n",
    "    if pd.isna(val):\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    # try JSON/python-literal list first\n",
    "    try:\n",
    "        parsed = ast.literal_eval(s)\n",
    "        if isinstance(parsed, list):\n",
    "            return [str(x) for x in parsed]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # very permissive fallback: split by |,; (rarely needed)\n",
    "    import re as _re\n",
    "    if any(sep in s for sep in \"|;,\"):\n",
    "        return [t.strip() for t in _re.split(r\"[|;,]\", s) if t.strip()]\n",
    "    return [s] if s else []\n",
    "\n",
    "# ----------------------------\n",
    "# Mentioned raw score sum per row\n",
    "# ----------------------------\n",
    "def mentioned_raw_sum(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Sum of RAW candidate scores for the candidates that were mentioned in the LLM response.\n",
    "    Range theoretically 0..1000 (10 cands * 100), but practically lower.\n",
    "    \"\"\"\n",
    "    cand_scores = parse_candidate_scores(row.get(\"candidate_answers\", \"\"))\n",
    "    if not cand_scores:\n",
    "        return 0.0\n",
    "    canon_scores = { _canon(k): float(v) for k, v in cand_scores.items() }\n",
    "    mentioned = parse_mentioned(row.get(\"mentioned_cands\", []))\n",
    "    mentioned_set = { _canon(m) for m in (mentioned or []) }\n",
    "    return float(sum(canon_scores[k] for k in canon_scores.keys() if k in mentioned_set))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0727ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows with NaN CI values\n",
      "[RAW] Pearson r: 0.2538125671765854 p: 3.1133278777848485e-25\n",
      "[RAW] Spearman ρ: 0.2046483874383687 p: 8.887830564597392e-17\n",
      "[NORM] Pearson r: 0.2538125671765854 p: 3.1133278777848485e-25\n",
      "[NORM] Spearman ρ: 0.2046483874383687 p: 8.887830564597392e-17\n"
     ]
    }
   ],
   "source": [
    "# Ensure numeric types\n",
    "df[\"CI\"] = pd.to_numeric(df[\"CI\"], errors=\"coerce\")\n",
    "print(df[\"CI\"].isna().sum(), \"rows with NaN CI values\")\n",
    "\n",
    "# Compute raw mentioned sum\n",
    "df[\"mentioned_raw_sum\"] = df.apply(mentioned_raw_sum, axis=1)\n",
    "\n",
    "# Optional: normalize to match CI scale (not required for Pearson, but provided for convenience)\n",
    "df[\"mentioned_raw_sum_norm\"] = df[\"mentioned_raw_sum\"] / S_MAX\n",
    "\n",
    "# Drop rows with missing CI\n",
    "df = df.dropna(subset=[\"CI\"])\n",
    "\n",
    "# --- Correlations: CI vs RAW mentioned sum ---\n",
    "pearson_raw, p_raw = pearsonr(df[\"CI\"], df[\"mentioned_raw_sum\"])\n",
    "spearman_raw, sp_raw = spearmanr(df[\"CI\"], df[\"mentioned_raw_sum\"])\n",
    "\n",
    "print(\"[RAW] Pearson r:\", pearson_raw, \"p:\", p_raw)\n",
    "print(\"[RAW] Spearman ρ:\", spearman_raw, \"p:\", sp_raw)\n",
    "\n",
    "# --- (Optional) Correlations: CI vs NORMALIZED mentioned sum ---\n",
    "pearson_norm, p_norm = pearsonr(df[\"CI\"], df[\"mentioned_raw_sum_norm\"])\n",
    "spearman_norm, sp_norm = spearmanr(df[\"CI\"], df[\"mentioned_raw_sum_norm\"])\n",
    "\n",
    "print(\"[NORM] Pearson r:\", pearson_norm, \"p:\", p_norm)\n",
    "print(\"[NORM] Spearman ρ:\", spearman_norm, \"p:\", sp_norm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
